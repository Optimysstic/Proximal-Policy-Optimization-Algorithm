# -*- coding: utf-8 -*-
"""PPO.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SDRRV_dpBNAgly5VLSqLpr5lfSYkvJi3
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
import numpy as np

# define the state_dim and action_dim
state_dim = 4
action_dim = 2

class Actor(nn.Module):
  def __init__(self, state_dim, action_dim):
    super().__init__()
    self.net = nn.Sequential(
        nn.Linear(state_dim, 64),
        nn.Tanh(),
        nn.Linear(64, action_dim)
    )
  def forward(self, state):
    return self.net(state)

class Critic(nn.Module):
  def __init__(self, state_dim):
    super().__init__()
    self.net = nn.Sequential(
        nn.Linear(state_dim, 64),
        nn.Tanh(),
        nn.Linear(64, 1)
    )
  def forward(self,state):
    return self.net(state)

class PPO:
  def __init__(self, state_dim, action_dim, config):
    self.actor = Actor(state_dim, action_dim)
    self.critic = Critic(state_dim)
    self.actor_old = Actor(state_dim, action_dim)
    self.actor_old.load_state_dict(self.actor.state_dict())
    self.optimizer_actor = optim.Adam(self.actor.parameters(), lr = config['actor_lr'])
    self.optimizer_critic = optim.Adam(self.critic.parameters(), lr = config['critic_lr'])
    self.config = config
    self.action_dim = action_dim

  def update(self, states, actions, old_log_probs, advantages, returns):
    # convert to tensors
    states = torch.FloatTensor(states)
    actions = torch.LongTensor(actions)
    old_log_probs = torch.FloatTensor(old_log_probs)
    advantages = torch.FloatTensor(advantages)
    returns = torch.FloatTensor(returns)

    # minibatch updates
    for _ in range(self.config['K']):
      indices = np.arange(len(states))
      np.random.shuffle(indices)
      for start in range(0, len(states), self.config['M']):
        end = start + self.config['M']
        idx = indices[start:end]

        # get the minibatch
        minibatch_states = states[idx]
        minibatch_actions = actions[idx]
        minibatch_old_log_probs = old_log_probs[idx]
        minibacth_advantages = advantages[idx]
        minibatch_returns = returns[idx]

        # actor loss
        logits = self.actor(minibatch_states)
        dist = Categorical(logits = logits)
        probs = dist.probs
        log_probs = torch.log(probs + 1e-10).gather(1, minibatch_actions.unsqueeze(1)).squeeze() # use .gather() to select the correct probabilities and squeeze() to remove unnecesary dimension
        ratios = torch.exp(log_probs - minibatch_old_log_probs)
        surr_1 = ratios * minibacth_advantages
        surr_2 = torch.clamp(ratios, 1-self.config['epsilon'], 1+self.config['epsilon']) * minibacth_advantages
        policy_loss = -torch.min(surr_1, surr_2).mean()
        entropy = dist.entropy().mean()
        loss_actor = policy_loss - self.config['lambda'] * entropy

        # critic loss
        values = self.critic(minibatch_states).squeeze()
        loss_critic = nn.MSELoss()(values, minibatch_returns)

        # update
        self.optimizer_actor.zero_grad()
        loss_actor.backward()
        self.optimizer_actor.step()
        self.optimizer_critic.zero_grad()
        loss_critic.backward()
        self.optimizer_critic.step()

    # update old actor
    self.actor_old.load_state_dict(self.actor.state_dict())

"""Edge Cases"""

# simple smaple minibacth(M = 1)
config = {'M': 1, 'K': 3, 'lambda': 0.1, 'epsilon': 0.2, 'actor_lr': 0.001, 'critic_lr':0.001} # added actor_lr and critic_lr to config
PPO = PPO(state_dim, action_dim, config)

# zero entropy weight
config = {'lambda': 0.0} # this should no teffect gradients, check entropy calcualtion doesn't produce NaN

# constant rewards
# all rewards = 0.0
# advantages should be calculated correctly (might be negative)
# policy updates should still work without NaN

# large clipping range
config = {'epsilon': 1e-6}
# should effectively disable clipping
# compare with vanilla policy gradient behavior

# NaN/Inf handling
states = np.random.randn(10, state_dim)
states[0] = np.nan # inject bad data
# should throw error or handle gracefully

# extreme action probabilities
# all the actions except one have the probability 0
# should handle log(0) issues in log_prob calculations

# empty trajectories
states = []
actions = []
# should handle empty bacth before training loop

# numerical stability
# add small epsilon to probabilites where needed
config = {'epsilon': 1e-6}
states = np.random.randn(10, state_dim)
states[0] = np.nan

# gradient clipping
# add after backward() calls
torch.nn.utils.cllip_grad_norm_(self.actor.parameters(), 0.5)

# device handling
# add device - agnostic code
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
states = states.to(device)

# normalization
# add advantage normalization
advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

"""To test these edge cases rigorously:

1. create unit tests with pytest
2. implement gradient checking
3. use anomaly detection
"""

with torch.autograd.detect_anomaly():
  loss.backward()

# monitor tensor statistics
print(f"Ratio range: {ratios.min().item():.2f} - {ratios.max().item():.2f}")